


[{"content":"","date":"31 July 2024","externalUrl":null,"permalink":"/blogs/","section":"博客","summary":"","title":"博客","type":"blogs"},{"content":"回归函数:\n$$f_{w,b}(x) = wx + b$$ 代价函数:\n$$J(w,b) = \\frac{1}{2m}\\sum_{i=1}^m(f_{w,b}(x_i)-y_i)^2$$ 梯度下降法同时更新w和b:\n$$tmp_w=w-\\alpha\\frac{\\partial J(w,b)}{\\partial w}=w-\\alpha\\frac{1}{m}\\sum_{i=1}^m(f_{w,b}(x_i)-y_i)x_i$$ $$tmp_b=b-\\alpha\\frac{\\partial J(w,b)}{\\partial b}=b-\\alpha\\frac{1}{m}\\sum_{i=1}^m(f_{w,b}(x_i)-y_i)$$ $$w = tmp_w$$ $$b = tmp_b$$ \\(\\alpha\\)是学习率,用来控制下降幅度\n","date":"26 July 2024","externalUrl":null,"permalink":"/blogs/%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","section":"博客","summary":"回归函数: $$f_{w,b}(x) = wx + b$$ 代价函数: $$J(w,b) = \\frac{1}{2m}\\sum_{i=1}^m(f_{w,b}(x_i)-y_i)^2$$ 梯度下降法同时更新w和b","title":"一元线性回归","type":"blogs"},{"content":"","date":"26 July 2024","externalUrl":null,"permalink":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","section":"Tags","summary":"","title":"机器学习","type":"tags"},{"content":"","externalUrl":null,"permalink":"/cn/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/cn/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/cn/","section":"Pigking的博客","summary":"","title":"Pigking的博客","type":"page"},{"content":"","externalUrl":null,"permalink":"/cn/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/cn/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]