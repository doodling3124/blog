


[{"content":"","date":"26 July 2024","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"🐱","type":"authors"},{"content":"","date":"26 July 2024","externalUrl":null,"permalink":"/blogs/","section":"Blogs","summary":"","title":"Blogs","type":"blogs"},{"content":"回归函数:$$f_{w,b}(x) = wx + b$$ 代价函数:$$J(w,b) = \\frac{1}{2m}\\sum_{i=1}^m(f_{w,b}(x_i)-y_i)^2$$ 梯度下降法同时更新w和b:\n$$tmp_w=w-\\alpha\\frac{\\partial J(w,b)}{\\partial w}=w-\\alpha\\frac{1}{m}\\sum_{i=1}^m(f_{w,b}(x_i)-y_i)x_i$$ $$tmp_b=b-\\alpha\\frac{\\partial J(w,b)}{\\partial b}=b-\\alpha\\frac{1}{m}\\sum_{i=1}^m(f_{w,b}(x_i)-y_i)$$ $$w = tmp_w$$ $$b = tmp_b$$\n\\(\\alpha\\)是学习率,用来控制下降幅度\n","date":"26 July 2024","externalUrl":null,"permalink":"/blogs/%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","section":"Blogs","summary":"回归函数:$$f_{w,b}(x) = wx + b$$ 代价函数:$$J(w,b) = \\frac{1}{2m}\\sum_{i=1}^m(f_{w,b}(x_i)-y_i)^2$$ 梯度下降法同时更新w和b:","title":"一元线性回归","type":"blogs"},{"content":"","date":"26 July 2024","externalUrl":null,"permalink":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","section":"Tags","summary":"","title":"机器学习","type":"tags"},{"content":"","externalUrl":null,"permalink":"/en/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/en/","section":"Blowfish","summary":"","title":"Blowfish","type":"page"},{"content":"","externalUrl":null,"permalink":"/en/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/en/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/en/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]